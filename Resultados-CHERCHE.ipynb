{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "790501b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cherche import data, retrieve\n",
    "from elasticsearch import Elasticsearch\n",
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from string import punctuation\n",
    "import nltk\n",
    "from unicodedata import normalize\n",
    "from nltk.stem import RSLPStemmer\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import RegexpTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ce8c7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "caminho_query = \"dados-conle-anonimizado-assunto-notnull (1).csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1da4b4",
   "metadata": {},
   "source": [
    "# Base de dados câmara dos deputados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80c647da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_assunto= pd.read_csv(caminho_query, encoding='utf-8', delimiter=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43732db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_assunto = df_assunto.to_numpy()\n",
    "y,X = arr_assunto[:,0],arr_assunto[:,1]\n",
    "y = [i.strip() for i in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a5d49f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verificar(y,top_n):\n",
    "    for d in top_n:\n",
    "        if y == d:\n",
    "            return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c25bca8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avaliacaoRecall(isPreprocess):\n",
    "\n",
    "    quant_encontrado=0\n",
    "    quant_relevante =0\n",
    "    for l,x in zip(y,X):\n",
    "\n",
    "        tokenized_query3 = x                   \n",
    "        if isPreprocess:\n",
    "            tokenized_query3 = preprocess(x)\n",
    "    \n",
    "        scores = retriever(tokenized_query3)\n",
    "        top_n = [d[\"name\"] for d in scores] \n",
    "\n",
    "        \n",
    "        quant_relevante+=1\n",
    "        quant_encontrado+=verificar(l,top_n)\n",
    "    \n",
    "    recall = quant_encontrado / quant_relevante\n",
    "    print(\"R@20: \"+str(recall))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6a387e",
   "metadata": {},
   "source": [
    "# Utilizando elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d04c87af",
   "metadata": {},
   "outputs": [],
   "source": [
    "es = Elasticsearch(hosts=\"localhost:9200\", http_auth=('elastic', '_3egIk1UEsLOV4266NWo'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a28d020",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_teste =20\n",
    "index_es = \"experimento_word_n_gram\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb58d951",
   "metadata": {},
   "source": [
    "# Pré processamento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c997a618",
   "metadata": {},
   "source": [
    "## 1- Sem pré processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bd1d5c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = retrieve.Elastic(es=es,on=[\"content\"],index=index_es,key=\"name\",k=k_teste)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb0d439",
   "metadata": {},
   "source": [
    "Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "963b29f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R@20: 0.5423728813559322\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bedca7b",
   "metadata": {},
   "source": [
    "## 2- Letra mínuscula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "669427b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(txt):\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    terms = tokenizer.tokenize(txt.lower())\n",
    "    terms = \" \".join(terms)\n",
    "    return terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "01546d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = retrieve.Elastic(es=es,on=[\"pre_text_lowercase\"],index=index_es,key=\"name\",k=k_teste)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0342bfa",
   "metadata": {},
   "source": [
    "Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7d297f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R@20: 0.5288135593220339\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c185a38",
   "metadata": {},
   "source": [
    "## 3- Letra mínuscula + remoção de pontuação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "66aee8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(txt):\n",
    "    stopwords = list(punctuation)\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    terms = tokenizer.tokenize(txt.lower())\n",
    "    terms = [word for word in terms if word not in stopwords]\n",
    "    terms = \" \".join(terms)\n",
    "    return terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e2088704",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = retrieve.Elastic(es=es,on=[\"pre_text_lowercase_pontuacao\"],index=index_es,key=\"name\",k=k_teste)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7319f265",
   "metadata": {},
   "source": [
    "Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9717eb18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R@20: 0.5288135593220339\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b6c99d",
   "metadata": {},
   "source": [
    "## 4- Letra mínuscula + remoção de pontuação e remoção de acentuação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1848c582",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_acentos(txt):\n",
    "    return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "def preprocess(txt):\n",
    "    txt = _remove_acentos(txt)\n",
    "    stopwords = list(punctuation)\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    terms = tokenizer.tokenize(txt.lower())\n",
    "    terms = [word for word in terms if word not in stopwords]\n",
    "    terms = \" \".join(terms)\n",
    "    return terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "24a35a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = retrieve.Elastic(es=es,on=[\"pre_text_lowercase_pontuacao_acentuacao\"],index=index_es,key=\"name\",k=k_teste)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a35e429",
   "metadata": {},
   "source": [
    "Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "85889a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R@20: 0.5932203389830508\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a6bd63",
   "metadata": {},
   "source": [
    "## 5- Letra mínuscula + remoção de pontuação + remoção de acentuação e remoção de stopword\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f1bef925",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_acentos(txt):\n",
    "    return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "def preprocess(txt):\n",
    "    txt = _remove_acentos(txt)\n",
    "    stopwords = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "    stopwords.extend(list(punctuation))\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    terms = tokenizer.tokenize(txt.lower())\n",
    "    terms = [word for word in terms if word not in stopwords]\n",
    "    terms = \" \".join(terms)\n",
    "    return terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e76e30f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = retrieve.Elastic(es=es,on=[\"pre_text_lowercase_pontuacao_acentuacao_stopword\"],index=index_es,key=\"name\",k=k_teste)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd455c1",
   "metadata": {},
   "source": [
    "Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f2d30fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R@20: 0.5898305084745763\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c39c225",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61cbd12",
   "metadata": {},
   "source": [
    "## 6- Stemming (RSLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "276e6b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(txt):\n",
    "\n",
    "    stemmer = RSLPStemmer()\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    terms = tokenizer.tokenize(txt)\n",
    "    terms = [stemmer.stem(word) for word in terms]\n",
    "    terms = \" \".join(terms)\n",
    "    return terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "870dc2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = retrieve.Elastic(es=es,on=[\"text_rslp\"],index=index_es,key=\"name\",k=k_teste)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887a16d2",
   "metadata": {},
   "source": [
    "Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4657a4a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R@20: 0.5627118644067797\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92b893c",
   "metadata": {},
   "source": [
    "## 7- Stemming (RSLP-S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dafa24f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RSLP_S:\n",
    "    def __plural_reduction(self, word):\n",
    "        excep = [\"lápis\",\"cais\",\"mais\",\"crúcis\",\"biquínis\",\"pois\",\"depois\",\"dois\",\"leis\" ]\n",
    "        excep_s = [\"aliás\",\"pires\",\"lápis\",\"cais\",\"mais\",\"mas\",\"menos\", \"férias\",\"fezes\",\"pêsames\",\"crúcis\",\"gás\", \"atrás\",\"moisés\",\"através\",\"convés\",\"ês\", \"país\",\"após\",\"ambas\",\"ambos\",\"messias\"]\n",
    "\n",
    "        len_word = len(word)\n",
    "        new_word = list(word)\n",
    "\n",
    "        if len_word >= 3:\n",
    "            if new_word[-1] == 's' and new_word[-2] == 'n':\n",
    "                new_word[-2] = 'm'\n",
    "                sing = \"\".join(new_word)\n",
    "                sing = sing[:-1]\n",
    "                return sing\n",
    "\n",
    "            if new_word[-1] == 's' and new_word[-2] == 'e' and new_word[-3] == 'õ':\n",
    "                new_word[-3] = 'ã'\n",
    "                new_word[-2] = 'o'\n",
    "                sing = \"\".join(new_word)\n",
    "                sing = sing[:-1]\n",
    "                return  sing\n",
    "\n",
    "            if new_word[-1] == 's' and new_word[-2] == 'e' and new_word[-3] == 'ã':\n",
    "                if word == 'mães':\n",
    "                    word = word[:-1]\n",
    "                    return word\n",
    "                else:\n",
    "                    new_word[-2] = 'o'\n",
    "                    sing = \"\".join(new_word)\n",
    "                    sing = sing[:-1]\n",
    "                    return sing\n",
    "\n",
    "            if new_word[-1] == 's' and new_word[-2] == 'i' and new_word[-3] == 'a':\n",
    "                if word != 'cais' and word != 'mais':\n",
    "                    new_word[-2] = 'l'\n",
    "                    sing = \"\".join(new_word)\n",
    "                    sing = sing[:-1]\n",
    "                    return sing\n",
    "\n",
    "            if new_word[-1] == 's' and new_word[-2] == 'i' and new_word[-3] == 'é':\n",
    "                new_word[-3] = 'e'\n",
    "                new_word[-2] = 'l'\n",
    "                sing = \"\".join(new_word)\n",
    "                sing = sing[:-1]\n",
    "                return sing\n",
    "\n",
    "            if new_word[-1] == 's' and new_word[-2] == 'i' and new_word[-3] == 'e':\n",
    "                new_word[-3] = 'e'\n",
    "                new_word[-2] = 'l'\n",
    "                sing = \"\".join(new_word)\n",
    "                sing = sing[:-1]\n",
    "                return sing\n",
    "\n",
    "            if new_word[-1] == 's' and new_word[-2] == 'i' and new_word[-3] == 'ó':\n",
    "                new_word[-3] = 'o'\n",
    "                new_word[-2] = 'l'\n",
    "                sing = \"\".join(new_word)\n",
    "                sing = sing[:-1]\n",
    "                return sing\n",
    "\n",
    "            if new_word[-1] == 's' and new_word[-2] == 'i':\n",
    "                if word not in excep:\n",
    "                    new_word[-1] = 'l'\n",
    "                    sing = \"\".join(new_word)\n",
    "                    return sing\n",
    "\n",
    "            if new_word[-1] == 's' and new_word[-2] == 'e' and new_word[-3] == 'l':\n",
    "                word = word[:-2]\n",
    "                return word\n",
    "\n",
    "            if new_word[-1] == 's' and new_word[-2] == 'e' and new_word[-3] == 'r':\n",
    "                word = word[:-2]\n",
    "                return word\n",
    "\n",
    "            if new_word[-1] == 's':\n",
    "                if word not in excep_s:\n",
    "                    word = word[:-1]\n",
    "\n",
    "        return word\n",
    "\n",
    "    def stem(self, word):\n",
    "        word = self.__plural_reduction(word)\n",
    "\n",
    "        return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "14a97407",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(txt):\n",
    "\n",
    "    stemmer = RSLP_S()\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    terms = tokenizer.tokenize(txt)\n",
    "    terms = [stemmer.stem(word) for word in terms]\n",
    "    terms = \" \".join(terms)\n",
    "    return terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ffe1505f",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = retrieve.Elastic(es=es,on=[\"text_rslps\"],index=index_es,key=\"name\",k=k_teste)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e76cf2f",
   "metadata": {},
   "source": [
    "Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b553f2e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R@20: 0.5322033898305085\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead84019",
   "metadata": {},
   "source": [
    "## 8- Stemming (Savoy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6fce996",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Savoy:\n",
    "\n",
    "    def __removeAllPTAccent(self, old_word):\n",
    "        word = list(old_word)\n",
    "        len_word = len(word)-1\n",
    "        for i in range(len_word, -1, -1):\n",
    "            if word[i] == 'ä':\n",
    "                word[i] = 'a'\n",
    "            if word[i] == 'â':\n",
    "                word[i] = 'a'\n",
    "            if word[i] == 'à':\n",
    "                word[i] = 'a'\n",
    "            if word[i] == 'á':\n",
    "                word[i] = 'a'\n",
    "            if word[i] == 'ã':\n",
    "                word[i] = 'a'\n",
    "            if word[i] == 'ê':\n",
    "                word[i] = 'e'\n",
    "            if word[i] == 'é':\n",
    "                word[i] = 'e'\n",
    "            if word[i] == 'è':\n",
    "                word[i] = 'e'\n",
    "            if word[i] == 'ë':\n",
    "                word[i] = 'e'\n",
    "            if word[i] == 'ï':\n",
    "                word[i] = 'i'\n",
    "            if word[i] == 'î':\n",
    "                word[i] = 'i'\n",
    "            if word[i] == 'ì':\n",
    "                word[i] = 'i'\n",
    "            if word[i] == 'í':\n",
    "                word[i] = 'i'\n",
    "            if word[i] == 'ü':\n",
    "                word[i] = 'u'\n",
    "            if word[i] == 'ú':\n",
    "                word[i] = 'u'\n",
    "            if word[i] == 'ù':\n",
    "                word[i] = 'u'\n",
    "            if word[i] == 'û':\n",
    "                word[i] = 'u'\n",
    "            if word[i] == 'ô':\n",
    "                word[i] = 'o'\n",
    "            if word[i] == 'ö':\n",
    "                word[i] = 'o'\n",
    "            if word[i] == 'ó':\n",
    "                word[i] = 'o'\n",
    "            if word[i] == 'ò':\n",
    "                word[i] = 'o'\n",
    "            if word[i] == 'õ':\n",
    "                word[i] = 'o'\n",
    "            if word[i] == 'ç':\n",
    "                word[i] = 'c'\n",
    "\n",
    "        new_word = \"\".join(word)\n",
    "        return new_word\n",
    "\n",
    "    def __finalVowelPortuguese(self, word):\n",
    "        len_word = len(word)\n",
    "        if len_word > 3:\n",
    "            if word[-1] == 'e' or word[-1] == 'a' or word[-1] == 'o':\n",
    "                word = word[:-1]\n",
    "\n",
    "        return word\n",
    "\n",
    "    def __remove_PTsuffix(self, word):\n",
    "        len_word = len(word)\n",
    "\n",
    "        if len_word > 3:\n",
    "            if word[-1] == 's' and word[-2] == 'e' and (word[-3] == 'r' or word[-3] == 's' or word[-3] == 'z' or word[-3] == 'l'):\n",
    "                word = word[:-2]\n",
    "                return word\n",
    "        if len_word > 2:\n",
    "            if word[-1] == 's' and word[-2] == 'n':\n",
    "                new_word = list(word)\n",
    "                new_word[-2] = 'm'\n",
    "                sing = \"\".join(new_word)\n",
    "                sing = sing[:-1]\n",
    "                return sing\n",
    "\n",
    "        if len_word > 3:\n",
    "            if (word[-1] == 's' and word[-2] == 'i') and (word[-3] == 'e' or word[-3] == 'é'):\n",
    "                new_word = list(word)\n",
    "                new_word[-3] = 'e'\n",
    "                new_word[-2] = 'l'\n",
    "                sing = \"\".join(new_word)\n",
    "                sing = sing[:-1]\n",
    "                return sing\n",
    "\n",
    "        if len_word > 3:\n",
    "            if word[-1] == 's' and word[-2] == 'i' and word[-3] == 'a':\n",
    "                new_word = list(word)\n",
    "                new_word[-2] = 'l'\n",
    "                sing = \"\".join(new_word)\n",
    "                sing = sing[:-1]\n",
    "                return sing\n",
    "\n",
    "        if len_word > 3:\n",
    "            if word[-1] == 's' and word[-2] == 'i' and word[-3] == 'ó':\n",
    "                new_word = list(word)\n",
    "                new_word[-3] = 'o'\n",
    "                new_word[-2] = 'l'\n",
    "                sing = \"\".join(new_word)\n",
    "                sing = sing[:-1]\n",
    "                return sing\n",
    "\n",
    "        if len_word > 3:\n",
    "            if word[-1] == 's' and word[-2] == 'i':\n",
    "                new_word = list(word)\n",
    "                new_word[-1] = 'l'\n",
    "                sing = \"\".join(new_word)\n",
    "                return sing\n",
    "\n",
    "        if len_word > 2:\n",
    "            if word[-1] == 's' and word[-2] == 'e' and word[-3] == 'õ':\n",
    "                new_word = list(word)\n",
    "                new_word[-3] = 'ã'\n",
    "                new_word[-2] = 'o'\n",
    "                sing = \"\".join(new_word)\n",
    "                sing = sing[:-1]\n",
    "                return sing\n",
    "            if word[-1] == 's' and word[-2] == 'e' and word[-3] == 'ã':\n",
    "                new_word = list(word)\n",
    "                new_word[-2] = 'o'\n",
    "                sing = \"\".join(new_word)\n",
    "                sing = sing[:-1]\n",
    "                return sing\n",
    "\n",
    "        if len_word > 5:\n",
    "            if word[-1] == 'e' and word[-2] == 't' and word[-3] == 'n' and word[-4] == 'e' and word[-5] == 'm':\n",
    "                word = word[:-5]\n",
    "                return word\n",
    "\n",
    "        if len_word > 2:\n",
    "            if word[-1] == 's':\n",
    "                word = word[:-1]\n",
    "\n",
    "        return word\n",
    "\n",
    "    def __normFemininPortuguese(self, word):\n",
    "\n",
    "        len_word = len(word)\n",
    "\n",
    "        if len_word < 3 or word[-1] != 'a':\n",
    "            return word\n",
    "\n",
    "        if len_word > 6:\n",
    "\n",
    "            if word[-2] == 'h' and word[-3] == 'n' and word[-4] == 'i':\n",
    "                new_word = list(word)\n",
    "                new_word[-1] = 'o'\n",
    "                masc = \"\".join(new_word)\n",
    "                return masc\n",
    "\n",
    "            if word[-2] == 'c' and word[-3] == 'a' and word[-4] == 'i':\n",
    "                new_word = list(word)\n",
    "                new_word[-1] = 'o'\n",
    "                masc = \"\".join(new_word)\n",
    "                return masc\n",
    "\n",
    "            if word[-2] == 'r' and word[-3] == 'i' and word[-4] == 'e':\n",
    "                new_word = list(word)\n",
    "                new_word[-1] = 'o'\n",
    "                masc = \"\".join(new_word)\n",
    "                return masc\n",
    "\n",
    "        if len_word > 5:\n",
    "            if word[-2] == 'n' and word[-3] == 'o':\n",
    "                new_word = list(word)\n",
    "                new_word[-3] = 'ã'\n",
    "                new_word[-2] = 'o'\n",
    "                masc = \"\".join(new_word)\n",
    "                masc = masc[:-1]\n",
    "                return masc\n",
    "\n",
    "            if word[-2] == 'r' and word[-3] == 'o':\n",
    "                word = word[:-1]\n",
    "                return word\n",
    "\n",
    "            if word[-2] == 's' and word[-3] == 'o':\n",
    "                new_word = list(word)\n",
    "                new_word[-1] = 'o'\n",
    "                masc = \"\".join(new_word)\n",
    "                return masc\n",
    "\n",
    "            if word[-2] == 's' and word[-3] == 'e':\n",
    "                new_word = list(word)\n",
    "                new_word[-3] = 'ê'\n",
    "                masc = \"\".join(new_word)\n",
    "                masc = masc[:-1]\n",
    "                return masc\n",
    "\n",
    "            if word[-2] == 'c' and word[-3] == 'i':\n",
    "                new_word = list(word)\n",
    "                new_word[-1] = 'o'\n",
    "                masc = \"\".join(new_word)\n",
    "                return masc\n",
    "\n",
    "            if word[-2] == 'd' and word[-3] == 'i':\n",
    "                new_word = list(word)\n",
    "                new_word[-1] = 'o'\n",
    "                masc = \"\".join(new_word)\n",
    "                return masc\n",
    "\n",
    "            if word[-2] == 'd' and word[-3] == 'a':\n",
    "                new_word = list(word)\n",
    "                new_word[-1] = 'o'\n",
    "                masc = \"\".join(new_word)\n",
    "                return masc\n",
    "\n",
    "            if word[-2] == 'v' and word[-3] == 'i':\n",
    "                new_word = list(word)\n",
    "                new_word[-1] = 'o'\n",
    "                masc = \"\".join(new_word)\n",
    "                return masc\n",
    "\n",
    "            if word[-2] == 'm' and word[-3] == 'a':\n",
    "                new_word = list(word)\n",
    "                new_word[-1] = 'o'\n",
    "                masc = \"\".join(new_word)\n",
    "                return masc\n",
    "\n",
    "            if word[-2] == 'n':\n",
    "                new_word = list(word)\n",
    "                new_word[-1] = 'o'\n",
    "                masc = \"\".join(new_word)\n",
    "                return masc\n",
    "\n",
    "        return word\n",
    "\n",
    "    def stem(self, word):\n",
    "        len_word = len(word)\n",
    "        if len_word > 2:\n",
    "            word = self.__remove_PTsuffix(word)\n",
    "            word = self.__normFemininPortuguese(word)\n",
    "            word = self.__finalVowelPortuguese(word)\n",
    "            word = self.__removeAllPTAccent(word)\n",
    "\n",
    "        return word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "683bd355",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(txt):\n",
    "\n",
    "    stemmer = Savoy()\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    terms = tokenizer.tokenize(txt)\n",
    "    terms = [stemmer.stem(word) for word in terms]\n",
    "    terms = \" \".join(terms)\n",
    "    return terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6b0fb4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = retrieve.Elastic(es=es,on=[\"text_savoy\"],index=index_es,key=\"name\",k=k_teste)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188c975e",
   "metadata": {},
   "source": [
    "Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8953a409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R@20: 0.5254237288135594\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d105a0",
   "metadata": {},
   "source": [
    "## 9- Letra mínuscula + remoção de pontuação + remoção de acentuação e remoção de stopword + stemming (RSLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9b8ff101",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_acentos(txt):\n",
    "    return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "def preprocess(txt):\n",
    "    txt = _remove_acentos(txt)\n",
    "    stopwords = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "    stopwords.extend(list(punctuation))\n",
    "\n",
    "    stemmer = RSLPStemmer()\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    terms = tokenizer.tokenize(txt.lower())\n",
    "    terms = [stemmer.stem(word) for word in terms if word not in stopwords]\n",
    "        \n",
    "    return \" \".join(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bfd9884c",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = retrieve.Elastic(es=es,on=[\"pre_text_rslp\"],index=index_es,key=\"name\",k=k_teste)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745a12d8",
   "metadata": {},
   "source": [
    "Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f6364ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R@20: 0.6033898305084746\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e84efb",
   "metadata": {},
   "source": [
    "## 10- Letra mínuscula + remoção de pontuação + remoção de acentuação e remoção de stopword + stemming (RSLP-S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4f1ef2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_acentos(txt):\n",
    "    return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "def preprocess(txt):\n",
    "    txt = _remove_acentos(txt)\n",
    "    stopwords = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "    stopwords.extend(list(punctuation))\n",
    "\n",
    "    stemmer = RSLP_S()\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    terms = tokenizer.tokenize(txt.lower())\n",
    "    terms = [stemmer.stem(word) for word in terms if word not in stopwords]\n",
    "        \n",
    "    return \" \".join(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "43b917bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = retrieve.Elastic(es=es,on=[\"pre_text_rslps\"],index=index_es,key=\"name\",k=k_teste)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e917baed",
   "metadata": {},
   "source": [
    "Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c4854948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R@20: 0.5898305084745763\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81868d81",
   "metadata": {},
   "source": [
    "## 11- Letra mínuscula + remoção de pontuação + remoção de acentuação e remoção de stopword + stemming (Savoy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "50e3ea35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_acentos(txt):\n",
    "    return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "def preprocess(txt):\n",
    "    txt = _remove_acentos(txt)\n",
    "    stopwords = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "    stopwords.extend(list(punctuation))\n",
    "\n",
    "    stemmer = Savoy()\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    terms = tokenizer.tokenize(txt.lower())\n",
    "    terms = [stemmer.stem(word) for word in terms if word not in stopwords]\n",
    "        \n",
    "    return \" \".join(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "627ad1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = retrieve.Elastic(es=es,on=[\"pre_text_savoy\"],index=index_es,key=\"name\",k=k_teste)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2284bb",
   "metadata": {},
   "source": [
    "Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d3968db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R@20: 0.6169491525423729\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5e7859",
   "metadata": {},
   "source": [
    "# Word n-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7a9057",
   "metadata": {},
   "source": [
    "## 12- Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "25707f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(txt):\n",
    "\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    terms = tokenizer.tokenize(txt)\n",
    "    ngram = []\n",
    "    \n",
    "    ngram_2 = list(ngrams(terms, 2))\n",
    "        \n",
    "    for w in ngram_2:\n",
    "        string = w[0] + \" \" + w[1]\n",
    "        ngram.append(string)\n",
    "    \n",
    "    return \" \".join(ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "470a61ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = retrieve.Elastic(es=es,on=[\"text_bigram\"],index=index_es,key=\"name\",k=k_teste)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c06a690",
   "metadata": {},
   "source": [
    "Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b658e524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R@20: 0.5016949152542373\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e61b4d",
   "metadata": {},
   "source": [
    "## 13- Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a63da839",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(txt):\n",
    "\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    terms = tokenizer.tokenize(txt)\n",
    "    ngram = []\n",
    "    \n",
    "    ngram_3 = list(ngrams(terms, 3))\n",
    "        \n",
    "    for w in ngram_3:\n",
    "        string = w[0] + \" \" + w[1] + \" \" + w[2]\n",
    "        ngram.append(string)\n",
    "    \n",
    "    return \" \".join(ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9e4c7615",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = retrieve.Elastic(es=es,on=[\"text_trigram\"],index=index_es,key=\"name\",k=k_teste)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96119174",
   "metadata": {},
   "source": [
    "Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "55e2ea2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R@20: 0.48135593220338985\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3651c2f",
   "metadata": {},
   "source": [
    "## 14- Unigram + Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec9f404d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(txt):\n",
    "    \n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    terms = tokenizer.tokenize(txt)\n",
    "    \n",
    "    ngram = []\n",
    "    ngram_1 = list(ngrams(terms, 1))\n",
    "    ngram_2 = list(ngrams(terms, 2))\n",
    "    for w in ngram_1:\n",
    "        ngram.append(w[0])\n",
    "        \n",
    "    for w in ngram_2:\n",
    "        string = w[0] + \" \" + w[1]\n",
    "        ngram.append(string)\n",
    "    \n",
    "    return \" \".join(ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3f1bce66",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = retrieve.Elastic(es=es,on=[\"text_uni_bi\"],index=index_es,key=\"name\",k=k_teste)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b6f806",
   "metadata": {},
   "source": [
    "Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8c298415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R@20: 0.48135593220338985\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b68c6f",
   "metadata": {},
   "source": [
    "# Word n-gram + pré processamento básico"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17047989",
   "metadata": {},
   "source": [
    "## 15- Letra mínuscula + remoção de pontuação, acentuação e stopword + bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ad8251e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_acentos(txt):\n",
    "    return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "def preprocess(txt):\n",
    "    txt = _remove_acentos(txt)\n",
    "    stopwords = list(punctuation)\n",
    "\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    terms = tokenizer.tokenize(txt.lower())\n",
    "    terms = [word for word in terms if word not in stopwords]\n",
    "    \n",
    "    ngram = []\n",
    "    ngram_2 = list(ngrams(terms, 2))\n",
    "        \n",
    "    for w in ngram_2:\n",
    "        string = w[0] + \" \" + w[1]\n",
    "        ngram.append(string)\n",
    "    \n",
    "    return \" \".join(ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a481cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = retrieve.Elastic(es=es,on=[\"pre_text_bigram\"],index=index_es,key=\"name\",k=k_teste)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b54151",
   "metadata": {},
   "source": [
    "Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "515ec3dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R@20: 0.5627118644067797\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d3f4c9",
   "metadata": {},
   "source": [
    "## 16- Letra mínuscula + remoção de pontuação, acentuação e stopword + trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be470397",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_acentos(txt):\n",
    "    return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "def preprocess(txt):\n",
    "    txt = _remove_acentos(txt)\n",
    "    stopwords = list(punctuation)\n",
    "\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    terms = tokenizer.tokenize(txt.lower())\n",
    "    terms = [word for word in terms if word not in stopwords]\n",
    "    \n",
    "    ngram = []\n",
    "    ngram_3 = list(ngrams(terms, 3))\n",
    "        \n",
    "    for w in ngram_3:\n",
    "        string = w[0] + \" \" + w[1] + \" \" + w[2]\n",
    "        ngram.append(string)\n",
    "    \n",
    "    return \" \".join(ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62e2e6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = retrieve.Elastic(es=es,on=[\"pre_text_trigram\"],index=index_es,key=\"name\",k=k_teste)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0782a19",
   "metadata": {},
   "source": [
    "Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1393538b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R@20: 0.5389830508474577\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cdfe21",
   "metadata": {},
   "source": [
    "## 17- Letra mínuscula + remoção de pontuação, acentuação e stopword + unigram + bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d85b891",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_acentos(txt):\n",
    "    return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "def preprocess(txt):\n",
    "    txt = _remove_acentos(txt)\n",
    "    stopwords = list(punctuation)\n",
    "\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    terms = tokenizer.tokenize(txt.lower())\n",
    "    terms = [word for word in terms if word not in stopwords]\n",
    "    \n",
    "    ngram = []\n",
    "    ngram_1 = list(ngrams(terms, 1))\n",
    "    ngram_2 = list(ngrams(terms, 2))\n",
    "    for w in ngram_1:\n",
    "        ngram.append(w[0])\n",
    "        \n",
    "    for w in ngram_2:\n",
    "        string = w[0] + \" \" + w[1]\n",
    "        ngram.append(string)\n",
    "    \n",
    "    return \" \".join(ngram)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8dacd819",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = retrieve.Elastic(es=es,on=[\"pre_text_uni_bi\"],index=index_es,key=\"name\",k=k_teste)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275e3db9",
   "metadata": {},
   "source": [
    "Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a8100d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R@20: 0.5389830508474577\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4bb334",
   "metadata": {},
   "source": [
    "# Word n-gram + pré processamento básico + RSLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a489e9e1",
   "metadata": {},
   "source": [
    "## 18- Letra mínuscula + remoção de pontuação, acentuação e stopword + stemming (RSLP) + bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3fd32aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_acentos(txt):\n",
    "    return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "def preprocess(txt):\n",
    "    txt = _remove_acentos(txt)\n",
    "    stopwords = list(punctuation)\n",
    "\n",
    "    stemmer = RSLPStemmer()\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    terms = tokenizer.tokenize(txt.lower())\n",
    "    terms = [stemmer.stem(word) for word in terms if word not in stopwords]\n",
    "    \n",
    "    ngram = []\n",
    "    ngram_2 = list(ngrams(terms, 2))\n",
    "        \n",
    "    for w in ngram_2:\n",
    "        string = w[0] + \" \" + w[1]\n",
    "        ngram.append(string)\n",
    "    \n",
    "    return \" \".join(ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3f8f8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = retrieve.Elastic(es=es,on=[\"pre_text_bigram_rslp\"],index=index_es,key=\"name\",k=k_teste)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003e0431",
   "metadata": {},
   "source": [
    "Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de2d2c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R@20: 0.5491525423728814\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6118a280",
   "metadata": {},
   "source": [
    "## 19- Letra mínuscula + remoção de pontuação, acentuação e stopword + stemming (RSLP) + trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65521cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_acentos(txt):\n",
    "    return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "def preprocess(txt):\n",
    "    txt = _remove_acentos(txt)\n",
    "    stopwords = list(punctuation)\n",
    "\n",
    "    stemmer = RSLPStemmer()\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    terms = tokenizer.tokenize(txt.lower())\n",
    "    terms = [stemmer.stem(word) for word in terms if word not in stopwords]\n",
    "    \n",
    "    ngram = []\n",
    "    ngram_3 = list(ngrams(terms, 3))\n",
    "        \n",
    "    for w in ngram_3:\n",
    "        string = w[0] + \" \" + w[1] + \" \" + w[2]\n",
    "        ngram.append(string)\n",
    "    \n",
    "    return \" \".join(ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "768bf678",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = retrieve.Elastic(es=es,on=[\"pre_text_trigram_rslp\"],index=index_es,key=\"name\",k=k_teste)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af5b491",
   "metadata": {},
   "source": [
    "Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95797fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R@20: 0.5186440677966102\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3641092e",
   "metadata": {},
   "source": [
    "## 20- Letra mínuscula + remoção de pontuação, acentuação e stopword + stemming (RSLP) + unigram + bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c4062176",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_acentos(txt):\n",
    "    return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "def preprocess(txt):\n",
    "    txt = _remove_acentos(txt)\n",
    "    stopwords = list(punctuation)\n",
    "\n",
    "    stemmer = RSLPStemmer()\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    terms = tokenizer.tokenize(txt.lower())\n",
    "    terms = [stemmer.stem(word) for word in terms if word not in stopwords]\n",
    "    \n",
    "    ngram = []\n",
    "    ngram_1 = list(ngrams(terms, 1))\n",
    "    ngram_2 = list(ngrams(terms, 2))\n",
    "    for w in ngram_1:\n",
    "        ngram.append(w[0])\n",
    "        \n",
    "    for w in ngram_2:\n",
    "        string = w[0] + \" \" + w[1]\n",
    "        ngram.append(string)\n",
    "    \n",
    "    return \" \".join(ngram)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa50afb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = retrieve.Elastic(es=es,on=[\"pre_text_uni_bi_rslp\"],index=index_es,key=\"name\",k=k_teste)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4b2762",
   "metadata": {},
   "source": [
    "Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d3bb2bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R@20: 0.535593220338983\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb25077",
   "metadata": {},
   "source": [
    "# Word n-gram + pré processamento básico + RSLP-S"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5c1744",
   "metadata": {},
   "source": [
    "## 21- Letra mínuscula + remoção de pontuação, acentuação e stopword + stemming (RSLP-S) + bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39be8129",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_acentos(txt):\n",
    "    return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "def preprocess(txt):\n",
    "    txt = _remove_acentos(txt)\n",
    "    stopwords = list(punctuation)\n",
    "\n",
    "    stemmer = RSLP_S()\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    terms = tokenizer.tokenize(txt.lower())\n",
    "    terms = [stemmer.stem(word) for word in terms if word not in stopwords]\n",
    "    \n",
    "    ngram = []\n",
    "    ngram_2 = list(ngrams(terms, 2))\n",
    "        \n",
    "    for w in ngram_2:\n",
    "        string = w[0] + \" \" + w[1]\n",
    "        ngram.append(string)\n",
    "    \n",
    "    return \" \".join(ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a85b61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = retrieve.Elastic(es=es,on=[\"pre_text_bigram_rslps\"],index=index_es,key=\"name\",k=k_teste)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35cad03c",
   "metadata": {},
   "source": [
    "Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f4a06794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R@20: 0.5559322033898305\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e964dcbe",
   "metadata": {},
   "source": [
    "## 22- Letra mínuscula + remoção de pontuação, acentuação e stopword + stemming (RSLP-S) + trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d0b8b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_acentos(txt):\n",
    "    return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "def preprocess(txt):\n",
    "    txt = _remove_acentos(txt)\n",
    "    stopwords = list(punctuation)\n",
    "\n",
    "    stemmer = RSLP_S()\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    terms = tokenizer.tokenize(txt.lower())\n",
    "    terms = [stemmer.stem(word) for word in terms if word not in stopwords]\n",
    "    \n",
    "    ngram = []\n",
    "    ngram_3 = list(ngrams(terms, 3))\n",
    "        \n",
    "    for w in ngram_3:\n",
    "        string = w[0] + \" \" + w[1] + \" \" + w[2]\n",
    "        ngram.append(string)\n",
    "    \n",
    "    return \" \".join(ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aa92fa3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = retrieve.Elastic(es=es,on=[\"pre_text_trigram_rslps\"],index=index_es,key=\"name\",k=k_teste)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d26cf9",
   "metadata": {},
   "source": [
    "Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b492288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R@20: 0.5423728813559322\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68141820",
   "metadata": {},
   "source": [
    "## 23- Letra mínuscula + remoção de pontuação, acentuação e stopword + stemming (RSLP-S) + unigram + bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d25cc140",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_acentos(txt):\n",
    "    return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "def preprocess(txt):\n",
    "    txt = _remove_acentos(txt)\n",
    "    stopwords = list(punctuation)\n",
    "\n",
    "    stemmer = RSLP_S()\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    terms = tokenizer.tokenize(txt.lower())\n",
    "    terms = [stemmer.stem(word) for word in terms if word not in stopwords]\n",
    "    \n",
    "    ngram = []\n",
    "    ngram_1 = list(ngrams(terms, 1))\n",
    "    ngram_2 = list(ngrams(terms, 2))\n",
    "    for w in ngram_1:\n",
    "        ngram.append(w[0])\n",
    "        \n",
    "    for w in ngram_2:\n",
    "        string = w[0] + \" \" + w[1]\n",
    "        ngram.append(string)\n",
    "    \n",
    "    return \" \".join(ngram)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "32cc087b",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = retrieve.Elastic(es=es,on=[\"pre_text_uni_bi_rslps\"],index=index_es,key=\"name\",k=k_teste)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54689e37",
   "metadata": {},
   "source": [
    "Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1709a4d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R@20: 0.5423728813559322\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0197bc01",
   "metadata": {},
   "source": [
    "# Word n-gram + pré processamento básico + Savoy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f84b461",
   "metadata": {},
   "source": [
    "## 24- Letra mínuscula + remoção de pontuação, acentuação e stopword + stemming (Savoy) + bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b5f36ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_acentos(txt):\n",
    "    return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "def preprocess(txt):\n",
    "    txt = _remove_acentos(txt)\n",
    "    stopwords = list(punctuation)\n",
    "\n",
    "    stemmer = Savoy()\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    terms = tokenizer.tokenize(txt.lower())\n",
    "    terms = [stemmer.stem(word) for word in terms if word not in stopwords]\n",
    "    \n",
    "    ngram = []\n",
    "    ngram_2 = list(ngrams(terms, 2))\n",
    "        \n",
    "    for w in ngram_2:\n",
    "        string = w[0] + \" \" + w[1]\n",
    "        ngram.append(string)\n",
    "    \n",
    "    return \" \".join(ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ffa5381",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = retrieve.Elastic(es=es,on=[\"pre_text_bigram_savoy\"],index=index_es,key=\"name\",k=k_teste)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30334e1",
   "metadata": {},
   "source": [
    "Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8796297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R@20: 0.5796610169491525\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6f2ea1",
   "metadata": {},
   "source": [
    "## 25- Letra mínuscula + remoção de pontuação, acentuação e stopword + stemming (Savoy) + trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e0dc231",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_acentos(txt):\n",
    "    return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "def preprocess(txt):\n",
    "    txt = _remove_acentos(txt)\n",
    "    stopwords = list(punctuation)\n",
    "\n",
    "    stemmer = Savoy()\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    terms = tokenizer.tokenize(txt.lower())\n",
    "    terms = [stemmer.stem(word) for word in terms if word not in stopwords]\n",
    "    \n",
    "    ngram = []\n",
    "    ngram_3 = list(ngrams(terms, 3))\n",
    "        \n",
    "    for w in ngram_3:\n",
    "        string = w[0] + \" \" + w[1] + \" \" + w[2]\n",
    "        ngram.append(string)\n",
    "    \n",
    "    return \" \".join(ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "95c88dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = retrieve.Elastic(es=es,on=[\"pre_text_trigram_savoy\"],index=index_es,key=\"name\",k=k_teste)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649fe9cf",
   "metadata": {},
   "source": [
    "Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "42be2030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R@20: 0.5694915254237288\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76a92de",
   "metadata": {},
   "source": [
    "## 26- Letra mínuscula + remoção de pontuação, acentuação e stopword + stemming (Savoy) + unigram + bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1235c518",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_acentos(txt):\n",
    "    return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "def preprocess(txt):\n",
    "    txt = _remove_acentos(txt)\n",
    "    stopwords = list(punctuation)\n",
    "\n",
    "    stemmer = Savoy()\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    terms = tokenizer.tokenize(txt.lower())\n",
    "    terms = [stemmer.stem(word) for word in terms if word not in stopwords]\n",
    "    \n",
    "    ngram = []\n",
    "    ngram_1 = list(ngrams(terms, 1))\n",
    "    ngram_2 = list(ngrams(terms, 2))\n",
    "    for w in ngram_1:\n",
    "        ngram.append(w[0])\n",
    "        \n",
    "    for w in ngram_2:\n",
    "        string = w[0] + \" \" + w[1]\n",
    "        ngram.append(string)\n",
    "    \n",
    "    return \" \".join(ngram)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6ea4692c",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = retrieve.Elastic(es=es,on=[\"pre_text_uni_bi_savoy\"],index=index_es,key=\"name\",k=k_teste)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4748fcb",
   "metadata": {},
   "source": [
    "Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "18266d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R@20: 0.5864406779661017\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f49dd8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
